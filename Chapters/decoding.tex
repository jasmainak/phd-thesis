% Chapter 5

\chapter{Decoding visual motion from MEG}
\label{chapter:decoding}
\noindent\makebox[\linewidth]{\rule{0.75\paperwidth}{0.4pt}}
\noindent\makebox[\linewidth]{\rule{0.75\paperwidth}{0.4pt}}

\localtableofcontents % local toc

\noindent\makebox[\linewidth]{\rule{0.75\paperwidth}{0.4pt}}
\noindent\makebox[\linewidth]{\rule{0.75\paperwidth}{0.4pt}}
\newpage

\section{Introduction - Context}
In natural environments, coherent motion is a vital sensory cue that helps the brain individuate objects in the world. Seminal neurophysiological work has described neurons in the middle temporal (MT) lobe of monkeys that were selective to the direction of motion and scaled to the coherence level of visual motion~\cite{2britten1992analysis} . During a perceptual classification task, direction-selectivity can readily be decoded from the activity of neural populations in MT~\cite{3jazayeri2006optimal,4rust2006mt}. As visual motion processing relies on neural population codes, it is amenable to non-invasive functional human brain imaging such as fMRI or magnetoencephalography (MEG). Supervised learning techniques such as Multivariate Pattern Analysis (MVPA) are increasingly successful at characterizing where and when the neural analysis of stimuli such as visual orientation, motion direction or object classification is being realized~\cite{5kamitani2005decoding,6wessberg2000real,7haynes2006decoding,8poldrack2011inferring,9cichy2014resolving,10horikawa2013neural,11haynes2015primer,wardle2016perceptual,ritchie2016neural,nakamura2003human,handel2008selective,amano2006estimation}.

In one of the earliest fMRI studies using MVPA, the direction of motion was successfully decoded from hMT+ (human analog of MT or V5) activity~\cite{12beauchamp1997graded}) but also, and surprisingly, from visual cortices V1, V2, V3 and V4~\cite{13kamitani2006decoding}. The successful decoding of visual motion in V1, V3 and hMT+ has since been reported several times~\cite{5kamitani2005decoding,13kamitani2006decoding,14serences2007representation,15hogendoorn2013decoding,16van2014decoding}. In addition to the typical feedforward processing of visual information excepted in early visual cortices, the ability to decode visual motion from lower visual areas was interpreted as a marker of  feature-based attention when required by the task~\cite{13kamitani2006decoding} and an effect of top-down modulation of early feedforward processing for conscious perception~\cite{14serences2007representation}. However, whether brain decoding using MVPA captures the selectivity of neural populations or not has been a subject of debate on the interpretational weight given to decoding~\cite{11haynes2015primer,17carlson2015sensible,mostert2015dissociating,pitts2012visual}. Relevant to the current study, recent fMRI work has suggested that the sources of decoding in early visual areas may reflect the perceptual priors and biases of motion direction computation~\cite{18vintch2014cortical}.

To disambiguate the functional role of different brain regions in motion selectivity, characterizing the temporal unfolding of pattern classification within and across visual regions could be helpful. Here, we exploited the temporal sensitivity of MEG to find the latency at which sufficient information had been integrated to reach a stable classification boundary~\cite{20mitchell2008predicting,21ramkumar2013feature,22king2014characterizing}. 36 participants were recorded with MEG while performing a visual motion coherence discrimination task in which two intermingled clouds of visual dots (red and green random-dot-kinematograms) moved randomly on the screen until one of the clouds moved more coherently than the other one~\cite{23zilber2014supramodal} (Figure ~\ref{fig:fig1}A). Participants were asked to report which of the two populations became most coherent over time. Seven motion coherence levels were tested and a novel multivariate decoding approach combining ridge regression and a ranking metric was developed. Contrary to classical decoding approaches based on binary classifiers such as Support Vector Machines (SVM), a single decoder was estimated for all coherence levels, allowing robust parameter estimation despite high dimensional data. The ranking metric allowed taking into account the fact that visual motion coherence was an ordered variable~\cite{herbrich00ordinal,Joachims:2002}. This novel decoder was applied to brain activity recorded at the sensor level and to cortically-constrained source estimates. Using this decoding technique, we report the categorization of two separate brain states as a function of the degree of visual motion coherence. The categorization boundary matched participants' behavioral outcomes. Our results suggest that incorporating such decoding methods may be suitable to address questions relevant to predictive coding and perceptual decision-making.

\begin{figure}
\centering
	\includegraphics[width=0.7\textwidth,angle=270]{decoding/fig1_complete}
    \caption{{\bf Categorization Decoding.} A) One experimental trial in which participants discriminated  which of the red or green cloud of moving dots was most coherent. B) Left: simulated data (gray) were best modeled by ordinal (red) than by a linear (green) fit. Right: similarity matrix providing a score of the decoding performance for each pairwise comparisons. C) Significant time-resolved decoding of visual motion coherence levels were found at 100 to 600 ms (green) post-stimulus onset. D) Grand-average (n=36) similarity matrices in sensors (top), hMT+ (middle) and frontal-pole (bottom) for the selected time window. Distributions of behavioral perceptual thresholds (gray histogram) and the mean (dashed line). E) Correlation scores between each template and similarity matrix (black histograms), and likeliest boundary decoded from MEG data (dashed red line).}\label{fig:fig1}
\end{figure}


\section{Experimental design \& Participants}
\subsection*{Participants}
Thirty-six participants took part in the study (16 females, mean 22.1 +/- 2.2 y.o.). All were right-handed, had normal hearing and normal or corrected-to-normal vision. Prior to the experiment, all participants gave a written informed consent. All methods were carried out in accordance with relevant guidelines and regulations and by NeuroSpin (Gif-sur-Yvette, France). The study was conducted in agreement with the Declaration of Helsinki (2008) and was approved by the Ethics Committee on Human Research at Neurospin (Gif-sur-Yvette, France). 

\subsection*{Experimental design}
The MEG session consisted of twelve experimental blocks alternating between rest and task ~\cite{23zilber2014supramodal}. Here, we solely focused on the main experimental task blocks in which participants' performance on a visual motion coherence task was being assessed. During the task, one trial started with the presentation of a fixation cross followed by two intermixed clouds of dots or Random Dot Kinematograms (RDKs) (red and green), whose motion was fully incoherent. After a variable interval of 0.3 to 0.6 s, one of the two RDKs became more coherent than the other one (Figure~\ref{fig:fig1}A). The participant had to determine by button press which of the red or green RDKs became more coherent. Seven possible levels of visual motion coherence were tested (15\%, 25\%, 35\%, 45\%, 55\%, 75\%, or 95\%), randomly assigned to a colour and to a direction. Each participant was tested with 28 trials per visual coherence level.

\subsubsection*{Visual stimuli}
The red and green RDKs were individually calibrated to isoluminance. In order to prevent local tracking of dots, a white fixation cross was located at the center of a \SI{4}{\degree} gray disk mask. RDKs were presented within an annulus of \SI{4}{\degree}-\SI{15}{\degree} of visual angle. Dots had a radius of \SI{0.2}{\degree}. The flow of RDKs was 16.7 dots per deg$^2 \times$ sec with a speed of \SI{10}{\degree}/s. During the first 0.3 to 0.6 s of a given trial, both RDKs were incoherent (0\% of coherent motion). The duration of the incoherent phase was pseudo-randomized across each trial in order to increase the difficulty of the task by preventing participants’ expectation of the temporal onset coherent motion. After the incoherent phase, one RDK became more coherent than the other one for one second in one specific direction. The direction of coherent dots was comprised within an angle of \SI{45}{\degree}-\SI{90}{\degree} around the azimuth. At each frame, 5\% of all dots were randomly reassigned to new positions and incoherent dots to a new direction of motion. Dots going into collision in the next frame were also reassigned a new direction of motion (more details in ~\cite{23zilber2014supramodal}).


\subsubsection*{Psychophysical analysis}
The performance of each individual was averaged as a function of the seven degrees of visual motion coherence of the stimuli, irrespectively to the colour or direction of motion. The coherence discrimination threshold was set to 75\% of correctness for each individual’s data, as typically used in a two-alternative forced choice (2-AFC) paradigm, forcing participants to adopt the same decision criterion for all stimuli~\cite{GreenSwets}. Here, the 75\% detection threshold corresponds to chance level. They were then separately fitted to psychometric functions with the maximum-likelihood methodology (Psignifit~\cite{41wichmann2001psychometric}) which provided valid estimates of perceptual thresholds on a per individual basis (more details in ~\cite{23zilber2014supramodal}).

\section{MEG pre-processing \& source localization}
All data pre-processing and source-imaging were done according to well accepted MEG guidelines~\cite{42gross2013good}. Signal-Space-Separation (SSS) was performed on raw data using Maxfilter (Elekta-Neuromag~\cite{43taulu2006spatiotemporal}) to compensate for external magnetic interferences. MEG data were band-pass filtered (2 to 45 Hz), down-sampled to 250 Hz and epoched from -100 ms to 1000 ms relatively to the onset of RDK coherence. Trials that were contaminated by artifacts were rejected (e.g. peak-to-peak amplitude difference above 150 microvolts in EOG data) leaving 89\% of trials considered to have an appropriate signal-to-noise ratio. The cortically constrained source reconstruction was done using the dSPM method following the guidelines of the MNE software~\cite{44gramfort2014mne}. The entire pre-processing was done using MNE~\cite{45gramfort2013meg}.

\section{MEG decoding}
Decoding generally consists in predicting a target variable $y$
from one pattern of brain activity $x\in\RR^p$ among all possible patterns or brain states.
When the target can take a finite number $K$ of possible values, like a multi-class classification problem, we have that $y \in\{1,\dots,K\}$. Here, when $x$ were MEG signals, $p$ was the number of channels and time points used for the prediction.
When $x$ was the amplitude of cortical sources, $p$ corresponded to the number of source locations.
The first goal of this study was to estimate how well each pair of visual motion coherence level could be discriminated against each other. Considering that multi-class classification approaches do not take into account the ordinal nature of the target to predict, indeed predicting $1$ instead of $7$ is as bad as predicting $1$ instead of $2$ although the mistake is obviously smaller in the second case, we instead built a decoder which could yield a high pattern classification accuracy for distinguishable coherence levels, and a low pattern classification accuracy for nearby levels of visual motion coherence which were perceptually hard to differentiate (\textit{cf.} next two sections about the method).\\
The second goal of the study was to find whether separate categorical brain states (two or more) emerged following the presentation of the stimuli as a function of the seven levels of visual motion coherence. Specifically, the task of participants consisted in deciding whether the red or the green cloud of dots was most coherent as a function of coherence level. One working hypothesis was thus that at least one boundary delimiting a possible threshold between the neural activations induced by low \textit{vs.} high coherent motion would be found during decoding.

To address this question, we opted out of a regression model estimated jointly for all levels of coherence, and combined it with a ranking metric adapted to discrete and ordered targets. Although an alternative approach could have consisted in testing the incoherent portion of the stimuli against each level of visual coherence, this would have lead to a strongly imbalanced training dataset (\textit{i.e.} 196 incoherence trials for 28 trials per level of coherence) which is heavily problematic for MVPA classification approaches~\cite{He:2009:LID:1591901.1592322}. Specifically, with this formulation of the decoding, an inaccurate model which always predicts incoherence instead of coherence would have 85\% of accuracy due to the imbalanced dataset. The ranking technique proposed here does not suffer from such class imbalance, considering that a single regression model was learnt for all coherence levels, and the ranking metric employed yielded 50\% accuracy levels in spite of the low number of trials.\\

We now describe in detail the regression model employed.

\subsubsection*{Regression model}
Due to the limited number of data points available for learning, and to the high dimensional nature of the neuroimaging data, we used a linear model following the standard approach in MVPA studies
\cite{5kamitani2005decoding,20mitchell2008predicting,11haynes2015primer}. The target values $y\in\RR^n$, here provided for the $n$ data points available for statistical inference, were derived from a linear combination of data, $y=X\omega$, where $\omega\in\RR^p$ was a weight vector and $X$ was a $n$-by-$p$ data matrix. The value $n$ here corresponded to the number of stimuli presentations, a.k.a. single trials or epochs. For each $i^{th}$ observation, the target $y_i\in\{1,\dots,K\}$ could take $K$ different values: in this study, $K=7$ corresponded to the seven levels of visual motion coherence defining the number of classes. Again, a multi-class classification approach could have been adopted, yet this strategy would have ignored that target values were ordered. For instance, decoding the 5th instead of the 2nd level of motion coherence is worse than predicting the 3rd level of motion coherence instead of the 2nd one. This is an information that a multi-class linear SVM model could not exploit. An SVM would also estimate $p\times K$ parameters instead of $p$, which would have naturally increase the risk of overfitting and reduced the interpretability of the results.\\
Instead, we chose a ridge regression method, and evaluated the predictive performance with a metric tailored for ordinal problems. The ridge regression model was defined as the solution to the convex optimization problem:
\begin{equation}
	\hat{\omega} = \argmin_{\omega\in\RR^p}\|y-X\omega\|_2^2 + \lambda\|\omega\|_2^2.
\end{equation}
The ridge regression model is a popular approach, whose practical success is due to fast estimation, robustness to noise and limited sensitivity to rough tuning of the parameter $\lambda$. Indeed, results obtained by ridge regression are known to be far less sensitive to the choice of $\lambda$ parameter, compared to sparse estimators such as Lasso. In our experiments, $\lambda$ was the same for all subjects~\cite{Varoquaux2017166}.

Decoding was  performed on a per individual basis using all epochs. The 204 gradiometers and different time windows were tested: for example, for the time window ranging from 100 to 600 ms, the dimensions of the data were the number of samples $n=196$ (at most 28 trials x 7 coherence levels) depending on the number of dropped epochs times the number of features $p=204 \times 126 \sim 2.5 \times 10^4$, where the temporal window ranging from 100 ms to 600 ms contained up to 126 samples. The performance of the method was evaluated with a 10-fold stratified cross-validation which preserved the percentage of samples for each class or motion coherence level in each fold.

Decoding was also performed on source-reconstructed data in bilateral regions of interest (ROI), previously reported as being implicated in the task~\cite{23zilber2014supramodal}. In source-space, the dimensions of the data were $n=196$ at most and, for instance, $p=126\times 117 \sim 10^6$ depending on the size of the ROI (here, 117 dipoles in the ROI).

Following the estimation of the ridge regression model, a ranking metric was then employed to quantify the model performance while taking into account that the targets have a natural order.

\subsubsection*{Assessing decoding performance with pairwise ranking metric}
Although ridge regression preserves the order of the target variables, it does not provide a relevant metric for the evaluation of the success rate of the decoder with an ordered set of categories. When using a linear regression model, the mean square error (MSE) is the natural performance metric. Yet, in high dimensional settings with a limited number of samples $(n\ll p)$, as we are dealing with here, MSE is a poor metric. In order to reduce the variance of the estimated coefficients, high values of $\lambda$ were used, causing a strong amplitude bias on the coefficients and a poor performance when measured using MSE. Performance evaluated with MSE was also affected in the presence of a bimodal state as illustrated in Figure~\ref{fig:fig1}B. Note that this strong bias problem is what motivates certain authors to use a Pearson correlation as a measure of performance rather than the MSE, although MSE is natural when using ridge regression~\cite{kay2008}.

In order to leverage the ordinal nature of the target values $y$, we quantified the performance in terms of ranking, where we tested the ability of the decoder to properly order pairs of samples, trials, based on the target to predict~\cite{herbrich00ordinal,Joachims:2002}. The ranking metric consisted in comparing the real values of $y$ and the predicted ones.
Let us consider two trials from the validation dataset with $y_i \neq y_j$ and where $(y_i, y_j)$ denote their associated labels.

Let $\mathcal{P}=\{(i,j) \hspace{1mm} s.t. \hspace{1.5mm} y_i \neq y_j\}$ be the set of pairs with different labels. One quantifies the prediction accuracy $Acc$ with the percentage of correct orderings for pairs of trials:
\begin{equation}
	Acc = \#\{(i,j) \in\mathcal{P} \hspace{2mm} s.t. \hspace{2mm} (y_i - y_j)(y_i^{pred} - y_j^{pred}) > 0 \}
\end{equation}
For each pair of trials, there were two possible options and the chance level was therefore 50\%. This quantity is related to Kendall’s rank correlation metric~\cite{46kruskal1958ordinal} which can be seen as a non-parametric correlation measure. To go beyond average accuracy, a key insight of this work was to inspect for which pair of trials the decoder made a mistake. For this, we thus defined a 7-by-7 similarity matrix $M$:
\begin{equation}
	M_{y_i,y_j} = \frac{\#\{(i,j)\in\mathcal{P} \hspace{2mm} s.t. \hspace{2mm} (y_i-y_j)(y_i^{pred}-y_j^{pred}) > 0\}}{\#\{(m,n)\in\mathcal{P}, (y_m, y_n) = (y_i, y_j)\}}
\end{equation}
Each $M_{i,j}$ was a value between 0 and 1 that told us how well we could distinguish the level $i$ from the level $j$, 1 being the best; inversely, if the level $i$ was similar or close to the level $j$, this decoding value would be close to chance level 0.5. The matrix was symmetric since comparing the levels $i$ and $j$ or $j$ and $i$  provides the same score. Such matrices, that can be seen as confusion matrices adapted for our pairwise ranking metric, are presented in Figure~\ref{fig:fig1}D.

\subsubsection*{Criteria for decoding categorization}
Template matrices were defined for the discrete values of theoretically possible categorization into two brain states driven by the motion coherence levels, namely: 15\%, 25\%, 35\%, 45\%, 55\%, 75\%, or 95\%. Each matrix had an on/off pattern at a given threshold (\textit{e.g.} 55\%) with values of 0.5 (off) or 0.65 (on) in order to make it comparable to decoding scores obtained in similarity matrices. An example is provided in the black matrices of Figure~\ref{fig:fig1}E. The correlation between the empirical matrices (fully based on MEG data) and all the possible template matrices, as defined above, provided the selection criterion to decode a categorization pattern at a specific threshold. Specifically, for each empirical similarity matrix, the template matrix yielding the highest correlation score was considered as a good predictor of the participants' motion coherence thresholds, eliciting the choice boundary from MEG data indicated as a dashed vertical line in Figure~\ref{fig:fig1}D.

\section{Results \& Discussion}
\subsection*{Modeling of simulated data as a proof of concept}

First, we modeled typical behavioral profiles observed during a perceptual discrimination task by using simulated data (\textit{e.g.} ranging from 7\% to 92\% of coherence). The modeling allowed validating the use of an ordinal model which better fitted the data than a linear model (Figure~\ref{fig:fig1}B, left panel). As detailed above, the simulated trials were decoded using cross-validation by fitting a ridge regression to the training data and evaluating the performance of the model on all possible pairwise combinations of test trials.
The similarity matrix (Figure~\ref{fig:fig1}B, right panel), which represents the predictive power in distinguishing two coherence levels, was evaluated with a 10-fold stratified cross-validation method. Each entry in the similarity matrix shows how similar each coherence level is to another one; alternatively, each entry can also be interpreted as how well one coherence level can be distinguished from another using a linear multivariate statistical model. All pairwise comparisons given in the similarity matrix built an anti-diagonal pattern: the lighter blocks in the similarity matrix were coherence levels for which no differences in brain responses could be captured, yielding a decoding score at chance level. Conversely, the darker blocks (red) captured high decoding accuracy scores for which brain responses highly differed between two coherent motion \textit{e.g.}, brain responses to 7\% coherent motion were highly distinguishable from those obtained during the presentation of 92\% coherent motion. When comparing the neighboring levels 64\% and 78\% in Figure~\ref{fig:fig1}B-right panel, the high accuracy of decoding demonstrated a difference in brain activity patterns, reflecting a discontinuity in the activation profiles despite a progressive change in the visual motion coherence levels. The observed discontinuity or edge located between 50\% and 64\% of visual motion coherence revealed the presence of a categorical boundary.\\

\subsection*{Spatial selectivity of decoding categorization}
The appropriate time window for best decoding performance was established using time-resolved cross-validation techniques~\cite{21ramkumar2013feature}. The overall best decoding performance was obtained for latencies ranging from 100 ms to 600 ms post-motion coherence onset as illustrated in Figure~\ref{fig:fig1}C. The decoder was applied to MEG data in this time window on a per individual basis. Similarity matrices scored how well pairs of visual motion coherence could be distinguished, and then ordered, on the basis of brain activity. Figure~\ref{fig:fig1}D reports the similarity matrices computed on grand-average MEG data (n = 36 participants). Similarity matrices obtained for the MEG sensors (gradiometers) are reported in the top panel. Similarity matrices obtained for source-reconstructed estimates in the ROI hMT+ and in a control region “frontal pole” are provided in the middle and bottom panels, respectively. 

The similarity matrices obtained in sensor and hMT+ data showed two distinct categories as an anti-block-diagonal pattern: two light blocks of decoding score at chance level ($\sim$50\%) for close coherence levels (low levels: 15\%-45\% against themselves, high levels: 55\%-95\% against themselves), and two dark blocks of decoding score nearing $\sim$65\% for coherence levels that were apart, namely 15\%-45\% against 55\%-95\%. These results conform with the notion of perceptual categories, namely: visual motion coherence levels 45\% and 55\% were close from the point of view of the coherence level in visual stimulation, but distant in perceptual space with the former most likely classified as incoherent and the latter as coherent. The two brain states thus defined by the similarity matrix are compatible with categorical classification of the stimuli in this task. Specifically, visual motion coherence stimuli could either elicit a pattern consistent with not detecting the coherent signal in the display and not discriminating within the ensemble of stimuli whose coherence could not be detected (below the boundary) and detecting the coherent signal in the display but not discriminating within the ensemble of stimuli whose coherence could be detected (above the boundary).

To further investigate the link between brain activity at the single trial level and  behavioral outcomes, we systematically compared the boundary delimited by the decoding approach with the perceptual threshold obtained from psychometric fits. The mean perceptual threshold was obtained in the task from the previous study~\cite{23zilber2014supramodal} and shown here in the histogram over the 36 subjects (Figure~\ref{fig:fig1}D, bottom panel). The emerging categorical boundary at 45-55\% of visual motion coherence in both sensors and hMT+ (but not frontal pole) matched well the mean perceptual threshold observed behaviorally (black dotted line; Figure~\ref{fig:fig1}D). 

In order to establish a quantitative criterion for this observation, template matrices were constructed to model each theoretically possible perceptual threshold. Each template matrix was then correlated with each of the decoding similarity matrices obtained from empirical measurements (Figure~\ref{fig:fig1}E). The aim was to find the peak of the correlation between the template threshold and the emerging boundary. This procedure, which is similar in spirit to the Representational Similarity Analysis (RSA) approach~\cite{Kriegeskorte-etal:08,9cichy2014resolving}, insured that the decoding similarity matrix was not forced to look like any specific template matrix. The quantitative metric confirmed our qualitative assessment (Figure~\ref{fig:fig1}D). Specifically, the peaks of the correlations were found for template matrices corresponding to a mean perceptual threshold of 55\% in both MEG sensors and in source-reconstructed hMT+; the control ROI showed no selectivity.

\subsection*{Temporal accumulation selectivity of categorization decoding}
The spatiotemporal sensitivity of source-reconstructed MEG data was exploited to test at which latency sufficient information had been integrated to reach a reliable and stable classification pattern. To explicit the choice of the cumulative time window range, Figure~\ref{fig:fig2}A shows the grand average time course in response to the seven motion coherence levels over the 36 subjects in hMT+. As can be seen (Figure~\ref{fig:fig2}A) and as previously reported~\cite{23zilber2014supramodal}, main differences were located at these latencies although no clear categorization were visible in the time response. For this, scoring was established in a temporally cumulative manner from 100 ms post-motion coherence onset by adding the consecutive 50 ms time window to each previous one (Figure~\ref{fig:fig2}B) until 450\,ms. The decoder was applied to sensors and to source estimates in the regions of interest as well as additional cortical sources known to be involved in the task~\cite{23zilber2014supramodal}, namely: hMT+ and the control region frontal pole but also the medial primary and secondary visual cortices (V1/V2), the intraparietal sulcus (IPS) and ventrolateral prefrontal region (VLPFC) (Figure~\ref{fig:fig2}, bottom left). In Figure~\ref{fig:fig2}B, in which all similarity matrices are reported, two brain categories of coherence levels seemed to emerge. As one of the focuses was to link the decoding to the behavioral data, the black dotted lines illustrated the known average perceptual threshold  to find how well it fitted with the boundary found in the similarity matrices.

\begin{figure}
\centering
	\includegraphics[width=0.7\textwidth,angle=270]{decoding/fig2_complete}
    \caption{{\bf Temporal-accumulation decoding.} A) Grand average hMT+ time courses in response to the seven motion coherence levels. B) Grand average similarity matrices (n = 36) in sensors, MT, V1/V2,  IPS, VLPFC and frontal pole (top to bottom rows, respectively). Incremental decoding of the similarity matrices within the selected time window could be seen. Colored frames indicate the earliest decoding pattern capturing the perceptual thresholds (dashed lines) \textit{e.g.} 250 ms for MT. C) Each similarity matrix was correlated with the template matrix optimally capturing perceptual thresholds. Correlations were cumulatively performed over the full time course of brain responses.}\label{fig:fig2}
\end{figure}


Using reverse-inference, we selected the template matrix which corresponded to the known mean perceptual thresholds of the 36 participants. We then computed the correlations in specific cortical regions to capture an anatomic and temporal discrimination. The correlation scores between the perceptual templates and the similarity matrices in the different cortical regions are provided in Figure~\ref{fig:fig2}C. The stability of the similarity matrices (Figure~\ref{fig:fig2}B) and the plateau of correlations between the template and the similarity matrices (Figure~\ref{fig:fig2}C) were first reached in hMT+ followed by V1/V2 in occipital regions, IPS and VLPFC. The latency of optimal decoding was consistent with seminal neurophysiology work suggesting functional selectivity of motion computation in hMT+ which may also be indicative of behavioral choice boundary~\cite{3jazayeri2006optimal,4rust2006mt,14serences2007representation,30britten1996relationship}. 
Perceptual boundaries for motion coherence discrimination could also be decoded later on in regions implicated in the task (V1/V2, IPS and much later in VlPFC) but not in the control region. These observations suggest that the decoder was anatomically and temporally selective. Specifically, the sequence of decoding latencies suggests that the outcome of categorization computed in hMT+ may be forwarded downstream to V1/V2 – as a possible general mechanism contributing to plasticity - as well as VLPFC, as a likely consequence of perceptual decisions required by the task. The decision-related aspect was likely not encoded in low-level sensory areas, however the categorization pattern was still visible in hMT+ when appearing in VLPFC due to accumulation of evidence over the whole time range. Although one could argue that the emergence of these patterns over time are essentially due to longer integration windows for decoding, using sensors or hMT+ label yields a visible categorization pattern as early as 100ms.

\section{Conclusion}
In this study, we showed that brain decoding could classify brain states as a function of visual motion coherence during a discrimination task. The categorical boundary partitioning two brain states was consistent with the participants' discrimination performance as indexed by their perceptual thresholds. Specifically, while the decoder was at chance level in discriminating between two motion coherence levels within the same perceptual category (within perceived or within non-perceived levels of visual motion coherence), the decoder performed well in discriminating brain activity in response to motion coherence levels across different categories (across perceived and non-perceived levels of visual motion coherence). We discuss below the implications and limitations of our findings.

In the visual motion coherence discrimination task used here, the intermixed clouds of dots (or RDKs) were identifiable by two distinct parameters: their color (red or green) and the increased degree of motion coherence in one cloud as compared with the other one. The task required participants to identify the colour of the coherent cloud of dots. Although the employed stimuli were quite typical for visual motion tasks, a couple requirements set this task apart. Firstly, the selective feature in the display was the coherence of motion irrespectively of the direction of motion. This differed from feature-based attention tasks in which the relevant feature is the direction of coherent motion~\cite{31treue1999feature,32saproo2014attention}. Secondly, the task required the discrimination of two clouds of dots simultaneously presented and spatially intermingled; this was distinct from a previous decoding study in which the two populations were spatially segregated~\cite{14serences2007representation}. Nevertheless, and consistently with prior decoding work on visual motion processing~\cite{5kamitani2005decoding,13kamitani2006decoding,14serences2007representation,15hogendoorn2013decoding,16van2014decoding} the earliest robust decoding of motion coherence was found in hMT+, as well as V1/V2. Thirdly, the color of the most coherent cloud of dots was randomized on every trial; as such, the color feature was orthogonal to the task requirement although the participants effectively classified their responses as "red" or "green". Accordingly, the successful decoding for any given pair of RDK coherence levels reported here (\textit{cf.} cells in the decoding matrix being $>50\%$) captured information about motion coherence \textit{per se}, not its color or its direction.

The behavioral discrimination of continuous sensory information, such as coherent motion, requires the setting up of an internal criterion classifying sensory information into two or more categories~\cite{3jazayeri2006optimal,30britten1996relationship}. Seminal work has shown that visual motion coherence at which neural activity reaches 50\% of its maximum value can be estimated by means of a neurometric threshold~\cite{2britten1992analysis}. A similar approach has been used on MEG source estimates in this task, revealing the extent to which the neurometric thresholds computed in the local brain area hMT+ could effectively reflect the participants' discrimination of visual motion coherence ~\cite{23zilber2014supramodal}. While perceptual thresholds can be derived using several analytical steps and fitting procedures, we have shown that a multivariate decoder can directly capture the partitioning of brain activity as a function of the participants' performance by using a dedicated ranking metric associated with a template matrix correlated with the errors of the decoder when evaluated on left out test data. Our approach also showed that the partitioning of brain states fitting the perceptual thresholds at the population level could be found at different timings and at different cortical locations. Future ad-hoc investigations will focus on correlating individual perceptual thresholds and similarity matrices. Here, the individual similarity matrices were noisier, making it harder to interpret the outcomes. Hence, we compared conditions by  taking the average over 36 participants, which still provided distinct categorization patterns.\\

Additionally, we found that the more sensory evidence accumulated over time, the more stable and robust the average similarity matrices became both in sensors and in brain regions. The first stable decoding pattern emerged in hMT+ ($\sim$250\,ms), consistently with the known likelihood estimations and evidence accumulation of visual motion in this region and at this latency~\cite{2britten1992analysis,3jazayeri2006optimal}. By 300\,ms, a comparable decoding pattern was found in V1/V2, followed by IPS and by 450\,ms by VLPFC. The early decoding latencies found in posterior regions and the later latencies found in frontal regions were overall consistent with decoding accuracies reported in perceptual decoding studies. Visual awareness can typically be decoded early in occipital regions and lately in frontal areas~\cite{andersen2016occipital,mostert2015dissociating,de2011perceptual,salti2015distinct,king2016brain} . While the late decoding component is related to perceptual awareness, it can also reflect expectation, task requirements, and attentional selection~\cite{andersen2016occipital,mostert2015dissociating}.

The observed spatiotemporal sequencing and stabilization of peak decoding in regions implicated in the task (but not others, \textit{i.e.} control area frontal pole) suggest that the motion selectivity and choice probability computed in hMT+ could be passed on downstream to early visual cortices as well as to decision-related areas (IPS). Recent models of visual motion processing~\cite{4rust2006mt} and recent fMRI data~\cite{18vintch2014cortical} have suggested that perceptual priors in early visual cortices may be shaped on the basis of higher-levels computations. Both seminal and recent findings have suggested that attention and feature-selectivity may be crucial in the modulation of early sensory cortices~\cite{5kamitani2005decoding,14serences2007representation,32saproo2014attention}. Our MEG decoding results add to this literature by suggesting that selectivity to higher-order features computed in hMT+, such as motion coherence irrespective of direction or color may feedback to early visual cortices. These and other~\cite{15hogendoorn2013decoding,16van2014decoding} results also suggest that the classification boundaries computed in hMT+ may have lasting effects for the analysis of visual motion. In particular, and consistently with previous literature~\cite{5kamitani2005decoding,13kamitani2006decoding,14serences2007representation,15hogendoorn2013decoding,16van2014decoding}, the latency of the categorization pattern across brain regions suggests the possibility that information relevant to perceptual boundaries from hMT+ feedbacks to V1/V2 consistently with predictive coding models of visual processing~\cite{4rust2006mt,25rao1999predictive} and learning theories~\cite{26ahissar2004reverse,27gilbert2009perceptual,28sasaki2010advances,29roelfsema2010perceptual}.

Nevertheless, it is noteworthy that in the context of perceptual categorization tasks such as the one employed here, the dissociation between the perceptual and the decisional components are difficult to disentangle~\cite{mostert2015dissociating,kelly2013internal,heekeren2008neural}. Several studies have discussed the dissociation between perceptual processing and decision-making~\cite{philiastides2006temporal,philiastides2006neural,ratcliff2009quality,o2012supramodal,wyart2012rhythmic,de2013prestimulus,kelly2013internal,pitts2012visual,mostert2015dissociating}. For instance, a temporal dissociation between early sensory processing in occipital areas and decision-related processing in parieto-frontal regions have been shown to be increasingly pronounced over time~\cite{mostert2015dissociating}. The perceptual thresholds used here to model the best fitting category do not readily dissociate between these two possibilities. Although the present study suggests that multivariate decoding can successfully retrieve perceptual thresholds, it is important to remain skeptical about the link between the information allowing decoding neural activity and its relationship to the computations effectively used to perform the task. For instance, brain activity categorized early on in hMT+ may contain top-down information feedback from decisional brain regions that may have helped the decoded categorization boundaries. However, three main aspects suggest that the decisional component may not be implicated here: firstly, the decision was made on the orthogonal feature color which was not used in the classifier as reported above. Secondly, the decoding in parietal cortices occurred much later than the stabilization observed in hMT+. Although response-locked analyses~\cite{kelly2013internal} could be used to disentangle the perceptual and decisional component, one limitation of the current decoder is that it is sensitive to any statistical differences in amplitude or in latency. Hence, analyzing the same time window sorted on the basis of the stimulus onset or of the response would not allow to draw stronger conclusions regarding the (perceptual or decisional) nature of the cortical representations enabling the categorization of brain states. Thirdly, recent evidence suggests that the inactivation of parietal regions are not decisive for motion categorization in monkeys~\cite{katz2016dissociated}. 

To sum up, we presented a new MEG decoding technique that can capture the perceived categorization of continuous sensory information at the population level. Our results showed a sustainable pattern over time that correlated with the mean perceptual threshold and which successively implicated hMT+, V1/V2, IPS and VLPFC, consistently with general models of decision-making in motion categorization tasks~\cite{mazurek2003role}. Future work will aim at disentangling the perceptual analysis and the decisional components of perceptual decision-making tasks, as well as refining our approach to individual-level decoding.
