%----------------------------------------------------------------------------------------
%	Introduction PAGE
%----------------------------------------------------------------------------------------

\addchaptertocentry{Introduction}

\newpage

{\huge\textbf{Introduction}\par}
\HRule \\[0.4cm] % Horizontal line
\section*{Context of the thesis or the Motivation}
%\textbf{Keywords:} Brain Imaging, Neuroscience Neurons, cells, Clinical work, cognitive science\\
%=> Non-invasive techniques/scanning - fMRI, M/EEG (functional)\\
%=> Why it is important to get back to the brain? Ex: Epilepsy for clinical case or more how the brain works for a cognitive task.

The brain is one of the most fascinating organ in a human being. It is able to represent, analyze, process, and transform information of millions of different tasks in a record time. It can go from language, perception, memory, attention, emotion to reasoning and creativity. Understanding how the brain works during these different tasks is important for several reasons. The first reason is on what consists cognitive science. The aim is to study the behavior of the brain and extract information to define a structure of each task. This has been widely used in another field known as  Artificial Intelligence where scientists try to implement aspects they learned from the human brain in computers. The second reason is more clinical, understanding how a pathology is affecting the brain, helps to find a cure or a way to improve patient life. For example, being able to detect autism in early age of childhood, helps the parents to have a specific education and a better future.\\

To make this brain screening possible, several techniques exists depending on the question we are really asking. For the different tasks I mentioned above, one very important aspect is time. They are mostly processed in a fraction of second, for example to recognize an emotion, to perceive a familiar face, etc. In this thesis, to study this high resolution of time, I was interested in two brain imaging techniques.\\

Magnetoencephalography (MEG) and electroencephalography (EEG) are functional neuro-imaging techniques for mapping the brain activity. They respectively record the magnetic and electric fields produced by electrical currents naturally occurring in the brain within the neurons. They use an array of sensors positioned over the scalp that are extremely sensitive to minuscule changes in the magnetic field (measured by MEG) produced by small changes in the electrical activity (measured by EEG) with the brain. It is, therefore, a direct measurement of neural activity. MEG/EEG as a technique for investigating the neural function in the brain is not new but was originally pioneered in the late 1960s. However, it is only since the early 1990s, with the introduction of high density detector grids covering the whole head, that the full potential of MEG has begun to be realized. The biggest advantage of MEG and EEG compared to fMRI which is much more established in the neuroscience research is the time resolution. In fMRI, neural activation is indirectly measured via local changes in the level of blood oxygenation, a long time window is typically compressed in one measured brain volume. \\

Using very sensitive magnetometers/electrodes (sensors), MEG and EEG deliver insight into the brain activity with high temporal and good spatial resolution. They allow the measurements of the ongoing activity which describe the active brain sources' state at each millisecond. This problem of estimating the result of the measurements is called \textit{forward problem}. The bioelectromagnetic forward problem describes the relationship between a given neural activity in the brain and the observable MEG and EEG signals. Its solution models mathematically the neural activity, the volume conductor, and the measurement setup. It allows to link the scalp potentials and external fields given an internal current distribution by a stable and unique solution, which is thus a well-posed problem. \\

Its counterpart, the bioelectromagnetic \textit{inverse problem} consists in using the actual measurements to infer the parameters (locations, amplitude, orientations) giving the distribution of the neural generators. It is an ill-posed problem in the sense of Hadamard due to its non-uniqueness and high sensitivity to noise, which makes its solution unstable. The inverse problem is the so called $n \ll p$ problem in machine learning, where you have much more unknowns to estimate than number of observations or variables. This problem has an infinite solution not only due to the small number of sensors (obervations $n$) present in the MEG and EEG. Actually, even if the MEG and EEG were measured simultaneously at infinitely many points over the head, the information will still be insufficient to uniquely compute the brain source distribution that generated the measured brain signals. This is due to the fact that there are different combinations of sources able to cause \textit{exactly} the same potential fields on the head. Thus, to infer the neural activity generating the data at the sensor level, different source reconstruction can be applied, which employ a priori knowledge on the sturcture of the brain activity in order to reduce the set of solutions to a unique one.\\

In the past twenty years, several lines of research have emerged to tackle the problem. The most common approach widely used in clinical application is the equivalent current dipole (ECD), which assumes the underlying neural sources to be focal, as for epileptic study. The limitation of the dipole fitting technique is its non-linearity, which makes the reconstruction challenging. Also, difficulties to accurately estimating the correct number of dipoles in advance. \\

Unlike dipole fitting method, distributed source models divide the source space into a grid containing a large number of possible dipoles (ECDs). The reconstruction of the source space is done simultaneously over all ECDs, which is less challenging when having correlated sources. However due to the large number of dipoles, the corresponding regression problem is undertermined, and then requires regularization. The regularization will define the type of a priori one needs to put on their solution, such as structural information, spatio-temporal characteristics of the source estimate. The disadvantage of these models compared to dipole fitting is that the solution is smeared, meaning it is not focal so harder for interpretation.\\
 
Nevertheless, the sparsity in the solution can be promoted with a specific type of regularization to the regression problem. The sparse models are actually the main interest of this thesis. They have been proposed in other fields of research and are widely used. In the signal processing literature, various signals can be defined as the linear combination of basis vectors, called \textit{atoms}. This technique is also known as compressed sensing. These atoms are defined in a fix overcomplete dictionary, the underlying motivation is that even though the observed signal is lying in a high-dimensional space, the actual signal is organized in a lower-dimensional space. This was used in the audio domain, specifically in the analysis of speech, sounds, and music, e.g. in order to classify a sound sample. The idea of sparse decomposition is also behind the JPEG2000 compression, which aims to keep only a few atoms best approximating the image. More in the image processing literature, the sparse models were used for denoising and image reconstruction (MRI,...). They are also linked to the dictionary learning literature, where one tries to learn a redundant and overcomplete dictionary, which is able to reconstruct a signal/image using a sparse setting.\\

\section*{Objective and scope}
%\textbf{Keywords:} Not only solve the IP but improve methods from the start to the end of the problem of source localization\\
%=> Sparse solutions (references), what people have been doing and how do we choosed to handle the problem\\
%=> Hyperparameter estimatio, to improve the tuning of the parameters which is important for solving the problem accurately.

In this thesis, I have been interested into the sparse models to reconstruct the source estimate for the MEG and EEG applications. Obtaining acceptable solutions, easy to interpret does not depend only on the a priori knownledge we impose, but several questions might be asked:
\begin{itemize}
\item How do we best fix the regularization to promote sparsity, such a way to obtain interpretable source estimates?
\item How do we set the hyperparameters of the regression problem?
\item How do we quantify the uncertainty of these obtained solutions?
\item How do we objectively compare the different existing solvers?
\end{itemize}
These points define the scope of this thesis, It tries to tackle firstly the problem of non-stationary sources, \textit{i.e.}, how to estimate a source that has a neuroscientific explanation as being active during a short time window only when looking a longer window. Second, this thesis tries to find a way to automatically estimate the hyperparameter of the regression problem to make comparison between solvers easier. Next step was to rewrite the problem as done by other communities: Bayesian formulation. This gave a way to reduce the gap between the variational and the Bayesian world by writing down their equivalence under a specific parametrization. The advantage of the Bayesian formulation is the ability to investigate the posterior distribution, making a study of solution's uncertainty possible. A third and last project of this thesis was to put together all the actual knowledge of source localization in MEG/EEG and have a complete study of their reconstruction on phantom dataset.

\section*{Contribution of the author}
This thesis presents novel approaches for source reconstruction in MEG/EEG. It can be divided into four main projects:
\begin{itemize}
\item The implementation of a widely known algorithm for MEG/EEG inverse problem, called Recursively Applied and projected (RAP) Music~\cite{mosher-leahy:1999}. The aim was to have a comparison with a state of the art sparse solver based on a non-convex regularization which promotes more sparsity by getting ride of all spurious sources. This work has been published in the \textbf{IEEE journal Transactions on Medical Imaging (TMI)}~\cite{strohmeier-etal:16}.

\item The improvement of a previous work of Daniel Strohmeier on source reconstruction in the time frequency domain , which resulted in the introduction of the TF-MxNE algorithm (Time-Frequency mixed-norm). The contribution tackles the problem of the choice of the dictionary where to decompose the data when working in the time-frequency domain. This consists on enabling the possibility to use combined dictionaries to make the algorithm able to find both transient and longer waveforms present in the brain signal. This work has been published in the \textbf{Pattern Recognition in NeuroImaging (PRNI)} in 2016~\cite{bekhti2016m}.

\item Different lines of research to solve the MEG/EEG inverse problem gave different formulations. The formulation mostly used in this thesis is a regularized regression as done in most machine learning problems. With this type of models, one needs to find a good compromise between the term that tries to fit the data called the data fit and the term regularizing the problem which takes into account any assumption one has onto the problem. This compromise is controlled by an external parameter usually called hyperparameter. For a practical example, when using sparse regularization, if this hyperparameter is fixed to a small value, \textit{i.e.}, the regularization term is not as important as the data fit, so the resulting solution will not be sparse enough and vice versa. Thus, the second contribution of this thesis was then to find an automated way to estimate this hypermarameter under some conditions of the model. This work has been published in the \textbf{European Signal Processing Conference (EUSIPCO)} in 2017~\cite{bekhti_eusipco}.

\item The biggest drawback of the sparse solvers is the fact that it gives one solution without any estimation of variance of any kind of confidence intervals. Some other application areas make use of Bayesian inference, mainly because it allows the estimation of uncertainty and its quantification is paramount. Therefore, the third contribution of this thesis is to rewrite to problem as done by in a Bayesian world, and tries to bridge this formulation with what has been presented so far. This chapter shows that under some condition, the Bayesian formulation and the variational one are equivalent. Then, it shows how we can get take advantage of the posterior distribution to extract uncertainty maps. This work has been submitted and is under review in the \textbf{IEEE journal Physics in Medicine \& Biology (PMB)}~\cite{bekhti_arxiv_pmb}.

\item The final project of this thesis is to test and validate several our solvers and several other ones that are mainly used for neuroscience application. This is done on phantom dataset which is simulated dataset with realistic environment as for a real humain brain. This work should be submitted soon to a journal paper.

\item An extra project on brain decoding is presented at the end of this thesis. This work presents a novel approach based on a ridge regression with a specific scorer that take into account ordered target. The approach is novel in terms of application to MEG data. This work has been submitted and is under review in the \textbf{journal Plos One}~\cite{Bekhti_bioarxiv}.

\item The implementation of some of the contribution is already on the MNE-python package~\cite{MNE}, the others should also be integrated soon. A latter minor contribution with a coworker project is published in both \textbf{Pattern Recognition in NeuroImaging}~\cite{jas_autoreject_prni} and \textbf{a journal Neuroimage}~\cite{jas_neuroimage}.
\end{itemize}

\section*{Structure of the thesis}
\begin{itemize}
\item \textbf{Chapter 1: Background and related work to the
M/EEG inverse problem}\\
This chapter defines the basics and the background needed for what will be presented in the rest of this thesis. It starts by giving the origin of the MEG and EEG recordings, \textit{i.e.}, what do the techniques really measure?. It gives then more insight on the forward operator and how it is computed efficiently. At this point, I present a full state of the art of inverse problems defining the three main approaches: beamforming or scanning techniques, image-based methods with distributed models, and sparse source models. Afterwards, I present some basics of time-frequency decomposition, and compare several dictionaries by giving their advantages. I finish this chapter by an optimization section, defining different ways to regularize the ill-posed problem and then how do we solve them. It also gives a comparison between several solvers.\\

\item \textbf{Chapter 2: Source localization with multi-scale dictionaries}\\
This chapter is dedicated to our first contribution, \textit{i.e.} solving the inverse problem in the time-frequency domain using a multi-scale dictionary. 
Source localization in the time-frequency domain has already been investigated using Gabor dictionary in a convex~\cite{Gramfort_Strohmeier_Haueisen_Hamalainen_Kowalski13} and a non-convex way~\cite{Strohmeier-etal:2015}. However, the choice of an optimal dictionary remains unsolved. Due to a mixture of signals, i.e. short transient signals (right after the stimulus onset) and slower brain waves, the choice of a single dictionary explaining simultaneously both signals types in a sparse way is difficult. this chapter introduces a method to improve the source estimation relying on a multi-scale dictionary, i.e. multiple dictionaries with different scales concatenated to fit short transients and slow waves at the same time. The benefits of this approach is shown in terms of reduced leakage (time courses mixture), temporal smoothness and detection of both signals types.

%Finally, the last chapter shows the experimental results obtained on simulated and on real MEG data. Then we discuss the achievements of this thesis and future work. 
%http://imaging.mrc-cbu.cam.ac.uk/meg/IntroEEGMEG

\item \textbf{Chapter 3: The Bayesian approach: Hierarchical Bayesian modeling}\\
This chapter gives the basic concepts of the Bayesian formulation of the MEG/EEG inverse problem. It also aims to explain the different jargon to link the variational and the Bayesian definitions. This ends up at defining an equivalence between the two communities under some conditions, while taking advantage of the Bayesian formulation which enables to study the multiple modes of the posterior distribution. The modes of the posterior will define several possible solutions to the inverse problem, allowing then the obtention of uncertainty maps of the source estimates. 

\item \textbf{Chapter 4: Benchmarking on phantom datasets}\\
This chapter is a validation chapter on phantom dataset. Phantom data is a dataset obtained by measuring the MEG/EEG activity with a humain skull phantom head. All real aspects of a head are simulated to generate the same conductivity which is due with a real skull. The dataset shown in this chapter has 4 simulated dipoles at different depth. With the knownledge of the groundtruth, this chapter studies the source reconstruction using several solvers of the inverse problem.

\item \textbf{Chapter 5: Decoding visual motion from MEG}\\
This chapter illustrates an extra project outside of the inverse problem topic. It is based on an application of machine learning to neuroscience. The aim was to develop an efficient approach to decode brain activity recorded with MEG while participants discriminated the coherence of two intermingled clouds of dots.
\end{itemize}
